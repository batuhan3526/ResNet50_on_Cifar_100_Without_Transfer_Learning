{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BIL95XzEIK4r"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GbUBH0aVIqPt"
   },
   "outputs": [],
   "source": [
    "# Normalize training set together with augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[x / 255.0 for x in[0.507, 0.487, 0.441]],\n",
    "                                     std=[x / 255.0 for x in [0.267, 0.256, 0.276]])\n",
    "])\n",
    "\n",
    "# Normalize test set same as training set without augmentation\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[x / 255.0 for x in[0.507, 0.487, 0.441]],\n",
    "                                     std=[x / 255.0 for x in [0.267, 0.256, 0.276]])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100,
     "referenced_widgets": [
      "8bffefb88a124d4a9c0c209b1bde1950",
      "74ff845e25604f548f47fea8b629a4aa",
      "17ab7edf3843410182a0e1b1d2f992fe",
      "a923df6caf654c10adc55ff9c519bb2d",
      "3dd48e39941c4b83a6bb5181fccf9554",
      "8b89d0abb7194008bbce622ed5687265",
      "a15ab6aea3ca43809d12366dd3563b2c",
      "fc5d9dfdd01546b281c589d9b3e93959"
     ]
    },
    "colab_type": "code",
    "id": "4b3Niu-XIqOR",
    "outputId": "c27db1d0-15d2-449f-cceb-75aa145dc6e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bffefb88a124d4a9c0c209b1bde1950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "learning_rate=0.001\n",
    "num_epochs = 40\n",
    "momentum=0.9\n",
    "weight_decay=1e-5\n",
    "batch_size_train=256\n",
    "batch_size_test=256\n",
    "batch_size = 256\n",
    "trainset = torchvision.datasets.CIFAR100(root = \"./data\",\n",
    "                                         train=True,\n",
    "                                         download=True,\n",
    "                                         transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=batch_size_train, shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root = \"./data\",\n",
    "                                        train=False,\n",
    "                                        download=True,\n",
    "                                        transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=batch_size_test, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "hgr934McIqKU",
    "outputId": "40589474-35b0-49ef-c5bd-ebde65f5c97e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda:0\n",
      "Tesla K80\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device: \",device)\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B0S09JjOIqIb"
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Basic Block for resnet 18 and resnet 34\n",
    "    \"\"\"\n",
    "\n",
    "    #BasicBlock and BottleNeck block\n",
    "    #have different output size\n",
    "    #we use class attribute expansion\n",
    "    #to distinct\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        #residual function\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "        )\n",
    "\n",
    "        #shortcut\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        #the shortcut output dimension is not the same with residual function\n",
    "        #use 1*1 convolution to match the dimension\n",
    "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.LeakyReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    \"\"\"Residual block for resnet over 50 layers\n",
    "    \"\"\"\n",
    "    expansion = 2\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, stride=stride, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BottleNeck.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.LeakyReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, num_block, num_classes=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(inplace=True))\n",
    "        #we use a different inputsize than the original paper\n",
    "        #so conv2_x's stride is 1\n",
    "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
    "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
    "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
    "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        \"\"\"make resnet layers(by layer i didnt mean this 'layer' was the\n",
    "        same as a neuron netowork layer, ex. conv layer), one layer may\n",
    "        contain more than one residual block\n",
    "        Args:\n",
    "            block: block type, basic block or bottle neck block\n",
    "            out_channels: output depth channel number of this layer\n",
    "            num_blocks: how many blocks per layer\n",
    "            stride: the stride of the first block of this layer\n",
    "        Return:\n",
    "            return a resnet layer\n",
    "        \"\"\"\n",
    "\n",
    "        # we have num_block blocks per layer, the first block\n",
    "        # could be 1 or 2, other blocks would always be 1\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2_x(output)\n",
    "        output = self.conv3_x(output)\n",
    "        output = self.conv4_x(output)\n",
    "        output = self.conv5_x(output)\n",
    "        output = self.avg_pool(output)\n",
    "        output = self.dropout(output)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "def resnet18():\n",
    "    \"\"\" return a ResNet 18 object\n",
    "    \"\"\"\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "def resnet34():\n",
    "    \"\"\" return a ResNet 34 object\n",
    "    \"\"\"\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "def resnet50():\n",
    "    \"\"\" return a ResNet 50 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck, [3, 4, 6, 3])\n",
    "\n",
    "def resnet101():\n",
    "    \"\"\" return a ResNet 101 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck, [3, 4, 23, 3])\n",
    "\n",
    "def resnet152():\n",
    "    \"\"\" return a ResNet 152 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck, [3, 8, 36, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1-IfSV0BIqF0"
   },
   "outputs": [],
   "source": [
    "model = resnet50()\n",
    "model = model.cuda()# I choose ResNet50. Because of Memory :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Lj6lNHtYsK3"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "EdCp0VouX-qA",
    "outputId": "c291b1d3-3c03-4a35-c2c1-1f7135c0a709"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\n",
      "conv1.0.weight \t 1728\n",
      "conv1.1.weight \t 64\n",
      "conv1.1.bias \t 64\n",
      "conv2_x.0.residual_function.0.weight \t 4096\n",
      "conv2_x.0.residual_function.1.weight \t 64\n",
      "conv2_x.0.residual_function.1.bias \t 64\n",
      "conv2_x.0.residual_function.3.weight \t 36864\n",
      "conv2_x.0.residual_function.4.weight \t 64\n",
      "conv2_x.0.residual_function.4.bias \t 64\n",
      "conv2_x.0.residual_function.6.weight \t 8192\n",
      "conv2_x.0.residual_function.7.weight \t 128\n",
      "conv2_x.0.residual_function.7.bias \t 128\n",
      "conv2_x.0.shortcut.0.weight \t 8192\n",
      "conv2_x.0.shortcut.1.weight \t 128\n",
      "conv2_x.0.shortcut.1.bias \t 128\n",
      "conv2_x.1.residual_function.0.weight \t 8192\n",
      "conv2_x.1.residual_function.1.weight \t 64\n",
      "conv2_x.1.residual_function.1.bias \t 64\n",
      "conv2_x.1.residual_function.3.weight \t 36864\n",
      "conv2_x.1.residual_function.4.weight \t 64\n",
      "conv2_x.1.residual_function.4.bias \t 64\n",
      "conv2_x.1.residual_function.6.weight \t 8192\n",
      "conv2_x.1.residual_function.7.weight \t 128\n",
      "conv2_x.1.residual_function.7.bias \t 128\n",
      "conv2_x.2.residual_function.0.weight \t 8192\n",
      "conv2_x.2.residual_function.1.weight \t 64\n",
      "conv2_x.2.residual_function.1.bias \t 64\n",
      "conv2_x.2.residual_function.3.weight \t 36864\n",
      "conv2_x.2.residual_function.4.weight \t 64\n",
      "conv2_x.2.residual_function.4.bias \t 64\n",
      "conv2_x.2.residual_function.6.weight \t 8192\n",
      "conv2_x.2.residual_function.7.weight \t 128\n",
      "conv2_x.2.residual_function.7.bias \t 128\n",
      "conv3_x.0.residual_function.0.weight \t 16384\n",
      "conv3_x.0.residual_function.1.weight \t 128\n",
      "conv3_x.0.residual_function.1.bias \t 128\n",
      "conv3_x.0.residual_function.3.weight \t 147456\n",
      "conv3_x.0.residual_function.4.weight \t 128\n",
      "conv3_x.0.residual_function.4.bias \t 128\n",
      "conv3_x.0.residual_function.6.weight \t 32768\n",
      "conv3_x.0.residual_function.7.weight \t 256\n",
      "conv3_x.0.residual_function.7.bias \t 256\n",
      "conv3_x.0.shortcut.0.weight \t 32768\n",
      "conv3_x.0.shortcut.1.weight \t 256\n",
      "conv3_x.0.shortcut.1.bias \t 256\n",
      "conv3_x.1.residual_function.0.weight \t 32768\n",
      "conv3_x.1.residual_function.1.weight \t 128\n",
      "conv3_x.1.residual_function.1.bias \t 128\n",
      "conv3_x.1.residual_function.3.weight \t 147456\n",
      "conv3_x.1.residual_function.4.weight \t 128\n",
      "conv3_x.1.residual_function.4.bias \t 128\n",
      "conv3_x.1.residual_function.6.weight \t 32768\n",
      "conv3_x.1.residual_function.7.weight \t 256\n",
      "conv3_x.1.residual_function.7.bias \t 256\n",
      "conv3_x.2.residual_function.0.weight \t 32768\n",
      "conv3_x.2.residual_function.1.weight \t 128\n",
      "conv3_x.2.residual_function.1.bias \t 128\n",
      "conv3_x.2.residual_function.3.weight \t 147456\n",
      "conv3_x.2.residual_function.4.weight \t 128\n",
      "conv3_x.2.residual_function.4.bias \t 128\n",
      "conv3_x.2.residual_function.6.weight \t 32768\n",
      "conv3_x.2.residual_function.7.weight \t 256\n",
      "conv3_x.2.residual_function.7.bias \t 256\n",
      "conv3_x.3.residual_function.0.weight \t 32768\n",
      "conv3_x.3.residual_function.1.weight \t 128\n",
      "conv3_x.3.residual_function.1.bias \t 128\n",
      "conv3_x.3.residual_function.3.weight \t 147456\n",
      "conv3_x.3.residual_function.4.weight \t 128\n",
      "conv3_x.3.residual_function.4.bias \t 128\n",
      "conv3_x.3.residual_function.6.weight \t 32768\n",
      "conv3_x.3.residual_function.7.weight \t 256\n",
      "conv3_x.3.residual_function.7.bias \t 256\n",
      "conv4_x.0.residual_function.0.weight \t 65536\n",
      "conv4_x.0.residual_function.1.weight \t 256\n",
      "conv4_x.0.residual_function.1.bias \t 256\n",
      "conv4_x.0.residual_function.3.weight \t 589824\n",
      "conv4_x.0.residual_function.4.weight \t 256\n",
      "conv4_x.0.residual_function.4.bias \t 256\n",
      "conv4_x.0.residual_function.6.weight \t 131072\n",
      "conv4_x.0.residual_function.7.weight \t 512\n",
      "conv4_x.0.residual_function.7.bias \t 512\n",
      "conv4_x.0.shortcut.0.weight \t 131072\n",
      "conv4_x.0.shortcut.1.weight \t 512\n",
      "conv4_x.0.shortcut.1.bias \t 512\n",
      "conv4_x.1.residual_function.0.weight \t 131072\n",
      "conv4_x.1.residual_function.1.weight \t 256\n",
      "conv4_x.1.residual_function.1.bias \t 256\n",
      "conv4_x.1.residual_function.3.weight \t 589824\n",
      "conv4_x.1.residual_function.4.weight \t 256\n",
      "conv4_x.1.residual_function.4.bias \t 256\n",
      "conv4_x.1.residual_function.6.weight \t 131072\n",
      "conv4_x.1.residual_function.7.weight \t 512\n",
      "conv4_x.1.residual_function.7.bias \t 512\n",
      "conv4_x.2.residual_function.0.weight \t 131072\n",
      "conv4_x.2.residual_function.1.weight \t 256\n",
      "conv4_x.2.residual_function.1.bias \t 256\n",
      "conv4_x.2.residual_function.3.weight \t 589824\n",
      "conv4_x.2.residual_function.4.weight \t 256\n",
      "conv4_x.2.residual_function.4.bias \t 256\n",
      "conv4_x.2.residual_function.6.weight \t 131072\n",
      "conv4_x.2.residual_function.7.weight \t 512\n",
      "conv4_x.2.residual_function.7.bias \t 512\n",
      "conv4_x.3.residual_function.0.weight \t 131072\n",
      "conv4_x.3.residual_function.1.weight \t 256\n",
      "conv4_x.3.residual_function.1.bias \t 256\n",
      "conv4_x.3.residual_function.3.weight \t 589824\n",
      "conv4_x.3.residual_function.4.weight \t 256\n",
      "conv4_x.3.residual_function.4.bias \t 256\n",
      "conv4_x.3.residual_function.6.weight \t 131072\n",
      "conv4_x.3.residual_function.7.weight \t 512\n",
      "conv4_x.3.residual_function.7.bias \t 512\n",
      "conv4_x.4.residual_function.0.weight \t 131072\n",
      "conv4_x.4.residual_function.1.weight \t 256\n",
      "conv4_x.4.residual_function.1.bias \t 256\n",
      "conv4_x.4.residual_function.3.weight \t 589824\n",
      "conv4_x.4.residual_function.4.weight \t 256\n",
      "conv4_x.4.residual_function.4.bias \t 256\n",
      "conv4_x.4.residual_function.6.weight \t 131072\n",
      "conv4_x.4.residual_function.7.weight \t 512\n",
      "conv4_x.4.residual_function.7.bias \t 512\n",
      "conv4_x.5.residual_function.0.weight \t 131072\n",
      "conv4_x.5.residual_function.1.weight \t 256\n",
      "conv4_x.5.residual_function.1.bias \t 256\n",
      "conv4_x.5.residual_function.3.weight \t 589824\n",
      "conv4_x.5.residual_function.4.weight \t 256\n",
      "conv4_x.5.residual_function.4.bias \t 256\n",
      "conv4_x.5.residual_function.6.weight \t 131072\n",
      "conv4_x.5.residual_function.7.weight \t 512\n",
      "conv4_x.5.residual_function.7.bias \t 512\n",
      "conv5_x.0.residual_function.0.weight \t 262144\n",
      "conv5_x.0.residual_function.1.weight \t 512\n",
      "conv5_x.0.residual_function.1.bias \t 512\n",
      "conv5_x.0.residual_function.3.weight \t 2359296\n",
      "conv5_x.0.residual_function.4.weight \t 512\n",
      "conv5_x.0.residual_function.4.bias \t 512\n",
      "conv5_x.0.residual_function.6.weight \t 524288\n",
      "conv5_x.0.residual_function.7.weight \t 1024\n",
      "conv5_x.0.residual_function.7.bias \t 1024\n",
      "conv5_x.0.shortcut.0.weight \t 524288\n",
      "conv5_x.0.shortcut.1.weight \t 1024\n",
      "conv5_x.0.shortcut.1.bias \t 1024\n",
      "conv5_x.1.residual_function.0.weight \t 524288\n",
      "conv5_x.1.residual_function.1.weight \t 512\n",
      "conv5_x.1.residual_function.1.bias \t 512\n",
      "conv5_x.1.residual_function.3.weight \t 2359296\n",
      "conv5_x.1.residual_function.4.weight \t 512\n",
      "conv5_x.1.residual_function.4.bias \t 512\n",
      "conv5_x.1.residual_function.6.weight \t 524288\n",
      "conv5_x.1.residual_function.7.weight \t 1024\n",
      "conv5_x.1.residual_function.7.bias \t 1024\n",
      "conv5_x.2.residual_function.0.weight \t 524288\n",
      "conv5_x.2.residual_function.1.weight \t 512\n",
      "conv5_x.2.residual_function.1.bias \t 512\n",
      "conv5_x.2.residual_function.3.weight \t 2359296\n",
      "conv5_x.2.residual_function.4.weight \t 512\n",
      "conv5_x.2.residual_function.4.bias \t 512\n",
      "conv5_x.2.residual_function.6.weight \t 524288\n",
      "conv5_x.2.residual_function.7.weight \t 1024\n",
      "conv5_x.2.residual_function.7.bias \t 1024\n",
      "fc.weight \t 102400\n",
      "fc.bias \t 100\n",
      "\n",
      "Total \t 16833700\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "print('Trainable parameters:')\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, '\\t', param.numel())\n",
    "        total += param.numel()\n",
    "print()\n",
    "print('Total', '\\t', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "HC8bfyAWYAN6",
    "outputId": "699aad29-6a48-433a-a1c0-dceca66bc885"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 0/196\n",
      "epoch: 0 7/196\n",
      "epoch: 0 14/196\n",
      "epoch: 0 21/196\n",
      "[1,    25] loss: 4.736\n",
      "epoch: 0 28/196\n",
      "epoch: 0 35/196\n",
      "epoch: 0 42/196\n",
      "[1,    50] loss: 4.358\n",
      "epoch: 0 49/196\n",
      "epoch: 0 56/196\n",
      "epoch: 0 63/196\n",
      "epoch: 0 70/196\n",
      "[1,    75] loss: 4.180\n",
      "epoch: 0 77/196\n",
      "epoch: 0 84/196\n",
      "epoch: 0 91/196\n",
      "epoch: 0 98/196\n",
      "[1,   100] loss: 4.031\n",
      "epoch: 0 105/196\n",
      "epoch: 0 112/196\n",
      "epoch: 0 119/196\n",
      "[1,   125] loss: 3.942\n",
      "epoch: 0 126/196\n",
      "epoch: 0 133/196\n",
      "epoch: 0 140/196\n",
      "epoch: 0 147/196\n",
      "[1,   150] loss: 3.825\n",
      "epoch: 0 154/196\n",
      "epoch: 0 161/196\n",
      "epoch: 0 168/196\n",
      "[1,   175] loss: 3.773\n",
      "epoch: 0 175/196\n",
      "epoch: 0 182/196\n",
      "epoch: 0 189/196\n",
      "Accuracy train 11 %\n",
      "Accuracy test 11 %\n",
      "epoch: 1 0/196\n",
      "epoch: 1 7/196\n",
      "epoch: 1 14/196\n",
      "epoch: 1 21/196\n",
      "[2,    25] loss: 6.805\n",
      "epoch: 1 28/196\n",
      "epoch: 1 35/196\n",
      "epoch: 1 42/196\n",
      "[2,    50] loss: 3.568\n",
      "epoch: 1 49/196\n",
      "epoch: 1 56/196\n",
      "epoch: 1 63/196\n",
      "epoch: 1 70/196\n",
      "[2,    75] loss: 3.537\n",
      "epoch: 1 77/196\n",
      "epoch: 1 84/196\n",
      "epoch: 1 91/196\n",
      "epoch: 1 98/196\n",
      "[2,   100] loss: 3.476\n",
      "epoch: 1 105/196\n",
      "epoch: 1 112/196\n",
      "epoch: 1 119/196\n",
      "[2,   125] loss: 3.411\n",
      "epoch: 1 126/196\n",
      "epoch: 1 133/196\n",
      "epoch: 1 140/196\n",
      "epoch: 1 147/196\n",
      "[2,   150] loss: 3.325\n",
      "epoch: 1 154/196\n",
      "epoch: 1 161/196\n",
      "epoch: 1 168/196\n",
      "[2,   175] loss: 3.245\n",
      "epoch: 1 175/196\n",
      "epoch: 1 182/196\n",
      "epoch: 1 189/196\n",
      "Accuracy train 21 %\n",
      "Accuracy test 19 %\n",
      "epoch: 2 0/196\n",
      "epoch: 2 7/196\n",
      "epoch: 2 14/196\n",
      "epoch: 2 21/196\n",
      "[3,    25] loss: 5.787\n",
      "epoch: 2 28/196\n",
      "epoch: 2 35/196\n",
      "epoch: 2 42/196\n",
      "[3,    50] loss: 3.058\n",
      "epoch: 2 49/196\n",
      "epoch: 2 56/196\n",
      "epoch: 2 63/196\n",
      "epoch: 2 70/196\n",
      "[3,    75] loss: 2.981\n",
      "epoch: 2 77/196\n",
      "epoch: 2 84/196\n",
      "epoch: 2 91/196\n",
      "epoch: 2 98/196\n",
      "[3,   100] loss: 2.949\n",
      "epoch: 2 105/196\n",
      "epoch: 2 112/196\n",
      "epoch: 2 119/196\n",
      "[3,   125] loss: 2.906\n",
      "epoch: 2 126/196\n",
      "epoch: 2 133/196\n",
      "epoch: 2 140/196\n",
      "epoch: 2 147/196\n",
      "[3,   150] loss: 2.829\n",
      "epoch: 2 154/196\n",
      "epoch: 2 161/196\n",
      "epoch: 2 168/196\n",
      "[3,   175] loss: 2.812\n",
      "epoch: 2 175/196\n",
      "epoch: 2 182/196\n",
      "epoch: 2 189/196\n",
      "Accuracy train 29 %\n",
      "Accuracy test 27 %\n",
      "epoch: 3 0/196\n",
      "epoch: 3 7/196\n",
      "epoch: 3 14/196\n",
      "epoch: 3 21/196\n",
      "[4,    25] loss: 5.002\n",
      "epoch: 3 28/196\n",
      "epoch: 3 35/196\n",
      "epoch: 3 42/196\n",
      "[4,    50] loss: 2.614\n",
      "epoch: 3 49/196\n",
      "epoch: 3 56/196\n",
      "epoch: 3 63/196\n",
      "epoch: 3 70/196\n",
      "[4,    75] loss: 2.608\n",
      "epoch: 3 77/196\n",
      "epoch: 3 84/196\n",
      "epoch: 3 91/196\n",
      "epoch: 3 98/196\n",
      "[4,   100] loss: 2.523\n",
      "epoch: 3 105/196\n",
      "epoch: 3 112/196\n",
      "epoch: 3 119/196\n",
      "[4,   125] loss: 2.521\n",
      "epoch: 3 126/196\n",
      "epoch: 3 133/196\n",
      "epoch: 3 140/196\n",
      "epoch: 3 147/196\n",
      "[4,   150] loss: 2.467\n",
      "epoch: 3 154/196\n",
      "epoch: 3 161/196\n",
      "epoch: 3 168/196\n",
      "[4,   175] loss: 2.420\n",
      "epoch: 3 175/196\n",
      "epoch: 3 182/196\n",
      "epoch: 3 189/196\n",
      "Accuracy train 37 %\n",
      "Accuracy test 33 %\n",
      "epoch: 4 0/196\n",
      "epoch: 4 7/196\n",
      "epoch: 4 14/196\n",
      "epoch: 4 21/196\n",
      "[5,    25] loss: 4.364\n",
      "epoch: 4 28/196\n",
      "epoch: 4 35/196\n",
      "epoch: 4 42/196\n",
      "[5,    50] loss: 2.282\n",
      "epoch: 4 49/196\n",
      "epoch: 4 56/196\n",
      "epoch: 4 63/196\n",
      "epoch: 4 70/196\n",
      "[5,    75] loss: 2.275\n",
      "epoch: 4 77/196\n",
      "epoch: 4 84/196\n",
      "epoch: 4 91/196\n",
      "epoch: 4 98/196\n",
      "[5,   100] loss: 2.248\n",
      "epoch: 4 105/196\n",
      "epoch: 4 112/196\n",
      "epoch: 4 119/196\n",
      "[5,   125] loss: 2.205\n",
      "epoch: 4 126/196\n",
      "epoch: 4 133/196\n",
      "epoch: 4 140/196\n",
      "epoch: 4 147/196\n",
      "[5,   150] loss: 2.188\n",
      "epoch: 4 154/196\n",
      "epoch: 4 161/196\n",
      "epoch: 4 168/196\n",
      "[5,   175] loss: 2.163\n",
      "epoch: 4 175/196\n",
      "epoch: 4 182/196\n",
      "epoch: 4 189/196\n",
      "Accuracy train 43 %\n",
      "Accuracy test 40 %\n",
      "epoch: 5 0/196\n",
      "epoch: 5 7/196\n",
      "epoch: 5 14/196\n",
      "epoch: 5 21/196\n",
      "[6,    25] loss: 3.912\n",
      "epoch: 5 28/196\n",
      "epoch: 5 35/196\n",
      "epoch: 5 42/196\n",
      "[6,    50] loss: 2.045\n",
      "epoch: 5 49/196\n",
      "epoch: 5 56/196\n",
      "epoch: 5 63/196\n",
      "epoch: 5 70/196\n",
      "[6,    75] loss: 2.020\n",
      "epoch: 5 77/196\n",
      "epoch: 5 84/196\n",
      "epoch: 5 91/196\n",
      "epoch: 5 98/196\n",
      "[6,   100] loss: 2.024\n",
      "epoch: 5 105/196\n",
      "epoch: 5 112/196\n",
      "epoch: 5 119/196\n",
      "[6,   125] loss: 1.995\n",
      "epoch: 5 126/196\n",
      "epoch: 5 133/196\n",
      "epoch: 5 140/196\n",
      "epoch: 5 147/196\n",
      "[6,   150] loss: 2.006\n",
      "epoch: 5 154/196\n",
      "epoch: 5 161/196\n",
      "epoch: 5 168/196\n",
      "[6,   175] loss: 1.954\n",
      "epoch: 5 175/196\n",
      "epoch: 5 182/196\n",
      "epoch: 5 189/196\n",
      "Accuracy train 47 %\n",
      "Accuracy test 43 %\n",
      "epoch: 6 0/196\n",
      "epoch: 6 7/196\n",
      "epoch: 6 14/196\n",
      "epoch: 6 21/196\n",
      "[7,    25] loss: 3.508\n",
      "epoch: 6 28/196\n",
      "epoch: 6 35/196\n",
      "epoch: 6 42/196\n",
      "[7,    50] loss: 1.879\n",
      "epoch: 6 49/196\n",
      "epoch: 6 56/196\n",
      "epoch: 6 63/196\n",
      "epoch: 6 70/196\n",
      "[7,    75] loss: 1.815\n",
      "epoch: 6 77/196\n",
      "epoch: 6 84/196\n",
      "epoch: 6 91/196\n",
      "epoch: 6 98/196\n",
      "[7,   100] loss: 1.887\n",
      "epoch: 6 105/196\n",
      "epoch: 6 112/196\n",
      "epoch: 6 119/196\n",
      "[7,   125] loss: 1.840\n",
      "epoch: 6 126/196\n",
      "epoch: 6 133/196\n",
      "epoch: 6 140/196\n",
      "epoch: 6 147/196\n",
      "[7,   150] loss: 1.848\n",
      "epoch: 6 154/196\n",
      "epoch: 6 161/196\n",
      "epoch: 6 168/196\n",
      "[7,   175] loss: 1.893\n",
      "epoch: 6 175/196\n",
      "epoch: 6 182/196\n",
      "epoch: 6 189/196\n",
      "Accuracy train 49 %\n",
      "Accuracy test 46 %\n",
      "epoch: 7 0/196\n",
      "epoch: 7 7/196\n",
      "epoch: 7 14/196\n",
      "epoch: 7 21/196\n",
      "[8,    25] loss: 3.338\n",
      "epoch: 7 28/196\n",
      "epoch: 7 35/196\n",
      "epoch: 7 42/196\n",
      "[8,    50] loss: 1.706\n",
      "epoch: 7 49/196\n",
      "epoch: 7 56/196\n",
      "epoch: 7 63/196\n",
      "epoch: 7 70/196\n",
      "[8,    75] loss: 1.742\n",
      "epoch: 7 77/196\n",
      "epoch: 7 84/196\n",
      "epoch: 7 91/196\n",
      "epoch: 7 98/196\n",
      "[8,   100] loss: 1.708\n",
      "epoch: 7 105/196\n",
      "epoch: 7 112/196\n",
      "epoch: 7 119/196\n",
      "[8,   125] loss: 1.715\n",
      "epoch: 7 126/196\n",
      "epoch: 7 133/196\n",
      "epoch: 7 140/196\n",
      "epoch: 7 147/196\n",
      "[8,   150] loss: 1.685\n",
      "epoch: 7 154/196\n",
      "epoch: 7 161/196\n",
      "epoch: 7 168/196\n",
      "[8,   175] loss: 1.666\n",
      "epoch: 7 175/196\n",
      "epoch: 7 182/196\n",
      "epoch: 7 189/196\n",
      "Accuracy train 55 %\n",
      "Accuracy test 49 %\n",
      "epoch: 8 0/196\n",
      "epoch: 8 7/196\n",
      "epoch: 8 14/196\n",
      "epoch: 8 21/196\n",
      "[9,    25] loss: 2.995\n",
      "epoch: 8 28/196\n",
      "epoch: 8 35/196\n",
      "epoch: 8 42/196\n",
      "[9,    50] loss: 1.561\n",
      "epoch: 8 49/196\n",
      "epoch: 8 56/196\n",
      "epoch: 8 63/196\n",
      "epoch: 8 70/196\n",
      "[9,    75] loss: 1.579\n",
      "epoch: 8 77/196\n",
      "epoch: 8 84/196\n",
      "epoch: 8 91/196\n",
      "epoch: 8 98/196\n",
      "[9,   100] loss: 1.597\n",
      "epoch: 8 105/196\n",
      "epoch: 8 112/196\n",
      "epoch: 8 119/196\n",
      "[9,   125] loss: 1.623\n",
      "epoch: 8 126/196\n",
      "epoch: 8 133/196\n",
      "epoch: 8 140/196\n",
      "epoch: 8 147/196\n",
      "[9,   150] loss: 1.596\n",
      "epoch: 8 154/196\n",
      "epoch: 8 161/196\n",
      "epoch: 8 168/196\n",
      "[9,   175] loss: 1.559\n",
      "epoch: 8 175/196\n",
      "epoch: 8 182/196\n",
      "epoch: 8 189/196\n",
      "Accuracy train 57 %\n",
      "Accuracy test 50 %\n",
      "epoch: 9 0/196\n",
      "epoch: 9 7/196\n",
      "epoch: 9 14/196\n",
      "epoch: 9 21/196\n",
      "[10,    25] loss: 2.830\n",
      "epoch: 9 28/196\n",
      "epoch: 9 35/196\n",
      "epoch: 9 42/196\n",
      "[10,    50] loss: 1.515\n",
      "epoch: 9 49/196\n",
      "epoch: 9 56/196\n",
      "epoch: 9 63/196\n",
      "epoch: 9 70/196\n",
      "[10,    75] loss: 1.491\n",
      "epoch: 9 77/196\n",
      "epoch: 9 84/196\n",
      "epoch: 9 91/196\n",
      "epoch: 9 98/196\n",
      "[10,   100] loss: 1.473\n",
      "epoch: 9 105/196\n",
      "epoch: 9 112/196\n",
      "epoch: 9 119/196\n",
      "[10,   125] loss: 1.459\n",
      "epoch: 9 126/196\n",
      "epoch: 9 133/196\n",
      "epoch: 9 140/196\n",
      "epoch: 9 147/196\n",
      "[10,   150] loss: 1.497\n",
      "epoch: 9 154/196\n",
      "epoch: 9 161/196\n",
      "epoch: 9 168/196\n",
      "[10,   175] loss: 1.492\n",
      "epoch: 9 175/196\n",
      "epoch: 9 182/196\n",
      "epoch: 9 189/196\n",
      "Accuracy train 60 %\n",
      "Accuracy test 54 %\n",
      "epoch: 10 0/196\n",
      "epoch: 10 7/196\n",
      "epoch: 10 14/196\n",
      "epoch: 10 21/196\n",
      "[11,    25] loss: 2.578\n",
      "epoch: 10 28/196\n",
      "epoch: 10 35/196\n",
      "epoch: 10 42/196\n",
      "[11,    50] loss: 1.382\n",
      "epoch: 10 49/196\n",
      "epoch: 10 56/196\n",
      "epoch: 10 63/196\n",
      "epoch: 10 70/196\n",
      "[11,    75] loss: 1.389\n",
      "epoch: 10 77/196\n",
      "epoch: 10 84/196\n",
      "epoch: 10 91/196\n",
      "epoch: 10 98/196\n",
      "[11,   100] loss: 1.414\n",
      "epoch: 10 105/196\n",
      "epoch: 10 112/196\n",
      "epoch: 10 119/196\n",
      "[11,   125] loss: 1.369\n",
      "epoch: 10 126/196\n",
      "epoch: 10 133/196\n",
      "epoch: 10 140/196\n",
      "epoch: 10 147/196\n",
      "[11,   150] loss: 1.392\n",
      "epoch: 10 154/196\n",
      "epoch: 10 161/196\n",
      "epoch: 10 168/196\n",
      "[11,   175] loss: 1.387\n",
      "epoch: 10 175/196\n",
      "epoch: 10 182/196\n",
      "epoch: 10 189/196\n",
      "Accuracy train 62 %\n",
      "Accuracy test 53 %\n",
      "epoch: 11 0/196\n",
      "epoch: 11 7/196\n",
      "epoch: 11 14/196\n",
      "epoch: 11 21/196\n",
      "[12,    25] loss: 2.497\n",
      "epoch: 11 28/196\n",
      "epoch: 11 35/196\n",
      "epoch: 11 42/196\n",
      "[12,    50] loss: 1.289\n",
      "epoch: 11 49/196\n",
      "epoch: 11 56/196\n",
      "epoch: 11 63/196\n",
      "epoch: 11 70/196\n",
      "[12,    75] loss: 1.294\n",
      "epoch: 11 77/196\n",
      "epoch: 11 84/196\n",
      "epoch: 11 91/196\n",
      "epoch: 11 98/196\n",
      "[12,   100] loss: 1.298\n",
      "epoch: 11 105/196\n",
      "epoch: 11 112/196\n",
      "epoch: 11 119/196\n",
      "[12,   125] loss: 1.280\n",
      "epoch: 11 126/196\n",
      "epoch: 11 133/196\n",
      "epoch: 11 140/196\n",
      "epoch: 11 147/196\n",
      "[12,   150] loss: 1.323\n",
      "epoch: 11 154/196\n",
      "epoch: 11 161/196\n",
      "epoch: 11 168/196\n",
      "[12,   175] loss: 1.333\n",
      "epoch: 11 175/196\n",
      "epoch: 11 182/196\n",
      "epoch: 11 189/196\n",
      "Accuracy train 65 %\n",
      "Accuracy test 55 %\n",
      "epoch: 12 0/196\n",
      "epoch: 12 7/196\n",
      "epoch: 12 14/196\n",
      "epoch: 12 21/196\n",
      "[13,    25] loss: 2.274\n",
      "epoch: 12 28/196\n",
      "epoch: 12 35/196\n",
      "epoch: 12 42/196\n",
      "[13,    50] loss: 1.201\n",
      "epoch: 12 49/196\n",
      "epoch: 12 56/196\n",
      "epoch: 12 63/196\n",
      "epoch: 12 70/196\n",
      "[13,    75] loss: 1.246\n",
      "epoch: 12 77/196\n",
      "epoch: 12 84/196\n",
      "epoch: 12 91/196\n",
      "epoch: 12 98/196\n",
      "[13,   100] loss: 1.220\n",
      "epoch: 12 105/196\n",
      "epoch: 12 112/196\n",
      "epoch: 12 119/196\n",
      "[13,   125] loss: 1.270\n",
      "epoch: 12 126/196\n",
      "epoch: 12 133/196\n",
      "epoch: 12 140/196\n",
      "epoch: 12 147/196\n",
      "[13,   150] loss: 1.229\n",
      "epoch: 12 154/196\n",
      "epoch: 12 161/196\n",
      "epoch: 12 168/196\n",
      "[13,   175] loss: 1.240\n",
      "epoch: 12 175/196\n",
      "epoch: 12 182/196\n",
      "epoch: 12 189/196\n",
      "Accuracy train 66 %\n",
      "Accuracy test 55 %\n",
      "epoch: 13 0/196\n",
      "epoch: 13 7/196\n",
      "epoch: 13 14/196\n",
      "epoch: 13 21/196\n",
      "[14,    25] loss: 2.239\n",
      "epoch: 13 28/196\n",
      "epoch: 13 35/196\n",
      "epoch: 13 42/196\n",
      "[14,    50] loss: 1.129\n",
      "epoch: 13 49/196\n",
      "epoch: 13 56/196\n",
      "epoch: 13 63/196\n",
      "epoch: 13 70/196\n",
      "[14,    75] loss: 1.120\n",
      "epoch: 13 77/196\n",
      "epoch: 13 84/196\n",
      "epoch: 13 91/196\n",
      "epoch: 13 98/196\n",
      "[14,   100] loss: 1.156\n",
      "epoch: 13 105/196\n",
      "epoch: 13 112/196\n",
      "epoch: 13 119/196\n",
      "[14,   125] loss: 1.191\n",
      "epoch: 13 126/196\n",
      "epoch: 13 133/196\n",
      "epoch: 13 140/196\n",
      "epoch: 13 147/196\n",
      "[14,   150] loss: 1.175\n",
      "epoch: 13 154/196\n",
      "epoch: 13 161/196\n",
      "epoch: 13 168/196\n",
      "[14,   175] loss: 1.176\n",
      "epoch: 13 175/196\n",
      "epoch: 13 182/196\n",
      "epoch: 13 189/196\n",
      "Accuracy train 67 %\n",
      "Accuracy test 57 %\n",
      "epoch: 14 0/196\n",
      "epoch: 14 7/196\n",
      "epoch: 14 14/196\n",
      "epoch: 14 21/196\n",
      "[15,    25] loss: 2.112\n",
      "epoch: 14 28/196\n",
      "epoch: 14 35/196\n",
      "epoch: 14 42/196\n",
      "[15,    50] loss: 1.074\n",
      "epoch: 14 49/196\n",
      "epoch: 14 56/196\n",
      "epoch: 14 63/196\n",
      "epoch: 14 70/196\n",
      "[15,    75] loss: 1.118\n",
      "epoch: 14 77/196\n",
      "epoch: 14 84/196\n",
      "epoch: 14 91/196\n",
      "epoch: 14 98/196\n",
      "[15,   100] loss: 1.117\n",
      "epoch: 14 105/196\n",
      "epoch: 14 112/196\n",
      "epoch: 14 119/196\n",
      "[15,   125] loss: 1.093\n",
      "epoch: 14 126/196\n",
      "epoch: 14 133/196\n",
      "epoch: 14 140/196\n",
      "epoch: 14 147/196\n",
      "[15,   150] loss: 1.128\n",
      "epoch: 14 154/196\n",
      "epoch: 14 161/196\n",
      "epoch: 14 168/196\n",
      "[15,   175] loss: 1.116\n",
      "epoch: 14 175/196\n",
      "epoch: 14 182/196\n",
      "epoch: 14 189/196\n",
      "Accuracy train 69 %\n",
      "Accuracy test 58 %\n",
      "epoch: 15 0/196\n",
      "epoch: 15 7/196\n",
      "epoch: 15 14/196\n",
      "epoch: 15 21/196\n",
      "[16,    25] loss: 1.974\n",
      "epoch: 15 28/196\n",
      "epoch: 15 35/196\n",
      "epoch: 15 42/196\n",
      "[16,    50] loss: 1.006\n",
      "epoch: 15 49/196\n",
      "epoch: 15 56/196\n",
      "epoch: 15 63/196\n",
      "epoch: 15 70/196\n",
      "[16,    75] loss: 1.056\n",
      "epoch: 15 77/196\n",
      "epoch: 15 84/196\n",
      "epoch: 15 91/196\n",
      "epoch: 15 98/196\n",
      "[16,   100] loss: 1.058\n",
      "epoch: 15 105/196\n",
      "epoch: 15 112/196\n",
      "epoch: 15 119/196\n",
      "[16,   125] loss: 1.043\n",
      "epoch: 15 126/196\n",
      "epoch: 15 133/196\n",
      "epoch: 15 140/196\n",
      "epoch: 15 147/196\n",
      "[16,   150] loss: 1.056\n",
      "epoch: 15 154/196\n",
      "epoch: 15 161/196\n",
      "epoch: 15 168/196\n",
      "[16,   175] loss: 1.049\n",
      "epoch: 15 175/196\n",
      "epoch: 15 182/196\n",
      "epoch: 15 189/196\n",
      "Accuracy train 70 %\n",
      "Accuracy test 58 %\n",
      "epoch: 16 0/196\n",
      "epoch: 16 7/196\n",
      "epoch: 16 14/196\n",
      "epoch: 16 21/196\n",
      "[17,    25] loss: 1.898\n",
      "epoch: 16 28/196\n",
      "epoch: 16 35/196\n",
      "epoch: 16 42/196\n",
      "[17,    50] loss: 0.989\n",
      "epoch: 16 49/196\n",
      "epoch: 16 56/196\n",
      "epoch: 16 63/196\n",
      "epoch: 16 70/196\n",
      "[17,    75] loss: 0.980\n",
      "epoch: 16 77/196\n",
      "epoch: 16 84/196\n",
      "epoch: 16 91/196\n",
      "epoch: 16 98/196\n",
      "[17,   100] loss: 0.987\n",
      "epoch: 16 105/196\n",
      "epoch: 16 112/196\n",
      "epoch: 16 119/196\n",
      "[17,   125] loss: 1.001\n",
      "epoch: 16 126/196\n",
      "epoch: 16 133/196\n",
      "epoch: 16 140/196\n",
      "epoch: 16 147/196\n",
      "[17,   150] loss: 1.005\n",
      "epoch: 16 154/196\n",
      "epoch: 16 161/196\n",
      "epoch: 16 168/196\n",
      "[17,   175] loss: 0.996\n",
      "epoch: 16 175/196\n",
      "epoch: 16 182/196\n",
      "epoch: 16 189/196\n",
      "Accuracy train 72 %\n",
      "Accuracy test 59 %\n",
      "epoch: 17 0/196\n",
      "epoch: 17 7/196\n",
      "epoch: 17 14/196\n",
      "epoch: 17 21/196\n",
      "[18,    25] loss: 1.753\n",
      "epoch: 17 28/196\n",
      "epoch: 17 35/196\n",
      "epoch: 17 42/196\n",
      "[18,    50] loss: 0.905\n",
      "epoch: 17 49/196\n",
      "epoch: 17 56/196\n",
      "epoch: 17 63/196\n",
      "epoch: 17 70/196\n",
      "[18,    75] loss: 0.922\n",
      "epoch: 17 77/196\n",
      "epoch: 17 84/196\n",
      "epoch: 17 91/196\n",
      "epoch: 17 98/196\n",
      "[18,   100] loss: 0.956\n",
      "epoch: 17 105/196\n",
      "epoch: 17 112/196\n",
      "epoch: 17 119/196\n",
      "[18,   125] loss: 0.932\n",
      "epoch: 17 126/196\n",
      "epoch: 17 133/196\n",
      "epoch: 17 140/196\n",
      "epoch: 17 147/196\n",
      "[18,   150] loss: 0.988\n",
      "epoch: 17 154/196\n",
      "epoch: 17 161/196\n",
      "epoch: 17 168/196\n",
      "[18,   175] loss: 0.943\n",
      "epoch: 17 175/196\n",
      "epoch: 17 182/196\n",
      "epoch: 17 189/196\n",
      "Accuracy train 74 %\n",
      "Accuracy test 60 %\n",
      "epoch: 18 0/196\n",
      "epoch: 18 7/196\n",
      "epoch: 18 14/196\n",
      "epoch: 18 21/196\n",
      "[19,    25] loss: 1.644\n",
      "epoch: 18 28/196\n",
      "epoch: 18 35/196\n",
      "epoch: 18 42/196\n",
      "[19,    50] loss: 0.854\n",
      "epoch: 18 49/196\n",
      "epoch: 18 56/196\n",
      "epoch: 18 63/196\n",
      "epoch: 18 70/196\n",
      "[19,    75] loss: 0.901\n",
      "epoch: 18 77/196\n",
      "epoch: 18 84/196\n",
      "epoch: 18 91/196\n",
      "epoch: 18 98/196\n",
      "[19,   100] loss: 0.883\n",
      "epoch: 18 105/196\n",
      "epoch: 18 112/196\n",
      "epoch: 18 119/196\n",
      "[19,   125] loss: 0.903\n",
      "epoch: 18 126/196\n",
      "epoch: 18 133/196\n",
      "epoch: 18 140/196\n",
      "epoch: 18 147/196\n",
      "[19,   150] loss: 0.934\n",
      "epoch: 18 154/196\n",
      "epoch: 18 161/196\n",
      "epoch: 18 168/196\n",
      "[19,   175] loss: 0.921\n",
      "epoch: 18 175/196\n",
      "epoch: 18 182/196\n",
      "epoch: 18 189/196\n",
      "Accuracy train 75 %\n",
      "Accuracy test 61 %\n",
      "epoch: 19 0/196\n",
      "epoch: 19 7/196\n",
      "epoch: 19 14/196\n",
      "epoch: 19 21/196\n",
      "[20,    25] loss: 1.608\n",
      "epoch: 19 28/196\n",
      "epoch: 19 35/196\n",
      "epoch: 19 42/196\n",
      "[20,    50] loss: 0.828\n",
      "epoch: 19 49/196\n",
      "epoch: 19 56/196\n",
      "epoch: 19 63/196\n",
      "epoch: 19 70/196\n",
      "[20,    75] loss: 0.864\n",
      "epoch: 19 77/196\n",
      "epoch: 19 84/196\n",
      "epoch: 19 91/196\n",
      "epoch: 19 98/196\n",
      "[20,   100] loss: 0.848\n",
      "epoch: 19 105/196\n",
      "epoch: 19 112/196\n",
      "epoch: 19 119/196\n",
      "[20,   125] loss: 0.848\n",
      "epoch: 19 126/196\n",
      "epoch: 19 133/196\n",
      "epoch: 19 140/196\n",
      "epoch: 19 147/196\n",
      "[20,   150] loss: 0.834\n",
      "epoch: 19 154/196\n",
      "epoch: 19 161/196\n",
      "epoch: 19 168/196\n",
      "[20,   175] loss: 0.862\n",
      "epoch: 19 175/196\n",
      "epoch: 19 182/196\n",
      "epoch: 19 189/196\n",
      "Accuracy train 76 %\n",
      "Accuracy test 61 %\n",
      "epoch: 20 0/196\n",
      "epoch: 20 7/196\n",
      "epoch: 20 14/196\n",
      "epoch: 20 21/196\n",
      "[21,    25] loss: 1.499\n",
      "epoch: 20 28/196\n",
      "epoch: 20 35/196\n",
      "epoch: 20 42/196\n",
      "[21,    50] loss: 0.794\n",
      "epoch: 20 49/196\n",
      "epoch: 20 56/196\n",
      "epoch: 20 63/196\n",
      "epoch: 20 70/196\n",
      "[21,    75] loss: 0.780\n",
      "epoch: 20 77/196\n",
      "epoch: 20 84/196\n",
      "epoch: 20 91/196\n",
      "epoch: 20 98/196\n",
      "[21,   100] loss: 0.785\n",
      "epoch: 20 105/196\n",
      "epoch: 20 112/196\n",
      "epoch: 20 119/196\n",
      "[21,   125] loss: 0.790\n",
      "epoch: 20 126/196\n",
      "epoch: 20 133/196\n",
      "epoch: 20 140/196\n",
      "epoch: 20 147/196\n",
      "[21,   150] loss: 0.800\n",
      "epoch: 20 154/196\n",
      "epoch: 20 161/196\n",
      "epoch: 20 168/196\n",
      "[21,   175] loss: 0.824\n",
      "epoch: 20 175/196\n",
      "epoch: 20 182/196\n",
      "epoch: 20 189/196\n",
      "Accuracy train 77 %\n",
      "Accuracy test 62 %\n",
      "epoch: 21 0/196\n",
      "epoch: 21 7/196\n",
      "epoch: 21 14/196\n",
      "epoch: 21 21/196\n",
      "[22,    25] loss: 1.421\n",
      "epoch: 21 28/196\n",
      "epoch: 21 35/196\n",
      "epoch: 21 42/196\n",
      "[22,    50] loss: 0.777\n",
      "epoch: 21 49/196\n",
      "epoch: 21 56/196\n",
      "epoch: 21 63/196\n",
      "epoch: 21 70/196\n",
      "[22,    75] loss: 0.777\n",
      "epoch: 21 77/196\n",
      "epoch: 21 84/196\n",
      "epoch: 21 91/196\n",
      "epoch: 21 98/196\n",
      "[22,   100] loss: 0.762\n",
      "epoch: 21 105/196\n",
      "epoch: 21 112/196\n",
      "epoch: 21 119/196\n",
      "[22,   125] loss: 0.773\n",
      "epoch: 21 126/196\n",
      "epoch: 21 133/196\n",
      "epoch: 21 140/196\n",
      "epoch: 21 147/196\n",
      "[22,   150] loss: 0.770\n",
      "epoch: 21 154/196\n",
      "epoch: 21 161/196\n",
      "epoch: 21 168/196\n",
      "[22,   175] loss: 0.804\n",
      "epoch: 21 175/196\n",
      "epoch: 21 182/196\n",
      "epoch: 21 189/196\n",
      "Accuracy train 79 %\n",
      "Accuracy test 61 %\n",
      "epoch: 22 0/196\n",
      "epoch: 22 7/196\n",
      "epoch: 22 14/196\n",
      "epoch: 22 21/196\n",
      "[23,    25] loss: 1.378\n",
      "epoch: 22 28/196\n",
      "epoch: 22 35/196\n",
      "epoch: 22 42/196\n",
      "[23,    50] loss: 0.721\n",
      "epoch: 22 49/196\n",
      "epoch: 22 56/196\n",
      "epoch: 22 63/196\n",
      "epoch: 22 70/196\n",
      "[23,    75] loss: 0.693\n",
      "epoch: 22 77/196\n",
      "epoch: 22 84/196\n",
      "epoch: 22 91/196\n",
      "epoch: 22 98/196\n",
      "[23,   100] loss: 0.697\n",
      "epoch: 22 105/196\n",
      "epoch: 22 112/196\n",
      "epoch: 22 119/196\n",
      "[23,   125] loss: 0.764\n",
      "epoch: 22 126/196\n",
      "epoch: 22 133/196\n",
      "epoch: 22 140/196\n",
      "epoch: 22 147/196\n",
      "[23,   150] loss: 0.730\n",
      "epoch: 22 154/196\n",
      "epoch: 22 161/196\n",
      "epoch: 22 168/196\n",
      "[23,   175] loss: 0.770\n",
      "epoch: 22 175/196\n",
      "epoch: 22 182/196\n",
      "epoch: 22 189/196\n",
      "Accuracy train 79 %\n",
      "Accuracy test 63 %\n",
      "epoch: 23 0/196\n",
      "epoch: 23 7/196\n",
      "epoch: 23 14/196\n",
      "epoch: 23 21/196\n",
      "[24,    25] loss: 1.271\n",
      "epoch: 23 28/196\n",
      "epoch: 23 35/196\n",
      "epoch: 23 42/196\n",
      "[24,    50] loss: 0.662\n",
      "epoch: 23 49/196\n",
      "epoch: 23 56/196\n",
      "epoch: 23 63/196\n",
      "epoch: 23 70/196\n",
      "[24,    75] loss: 0.695\n",
      "epoch: 23 77/196\n",
      "epoch: 23 84/196\n",
      "epoch: 23 91/196\n",
      "epoch: 23 98/196\n",
      "[24,   100] loss: 0.676\n",
      "epoch: 23 105/196\n",
      "epoch: 23 112/196\n",
      "epoch: 23 119/196\n",
      "[24,   125] loss: 0.704\n",
      "epoch: 23 126/196\n",
      "epoch: 23 133/196\n",
      "epoch: 23 140/196\n",
      "epoch: 23 147/196\n",
      "[24,   150] loss: 0.663\n",
      "epoch: 23 154/196\n",
      "epoch: 23 161/196\n",
      "epoch: 23 168/196\n",
      "[24,   175] loss: 0.733\n",
      "epoch: 23 175/196\n",
      "epoch: 23 182/196\n",
      "epoch: 23 189/196\n",
      "Accuracy train 80 %\n",
      "Accuracy test 63 %\n",
      "epoch: 24 0/196\n",
      "epoch: 24 7/196\n",
      "epoch: 24 14/196\n",
      "epoch: 24 21/196\n",
      "[25,    25] loss: 1.241\n",
      "epoch: 24 28/196\n",
      "epoch: 24 35/196\n",
      "epoch: 24 42/196\n",
      "[25,    50] loss: 0.655\n",
      "epoch: 24 49/196\n",
      "epoch: 24 56/196\n",
      "epoch: 24 63/196\n",
      "epoch: 24 70/196\n",
      "[25,    75] loss: 0.612\n",
      "epoch: 24 77/196\n",
      "epoch: 24 84/196\n",
      "epoch: 24 91/196\n",
      "epoch: 24 98/196\n",
      "[25,   100] loss: 0.633\n",
      "epoch: 24 105/196\n",
      "epoch: 24 112/196\n",
      "epoch: 24 119/196\n",
      "[25,   125] loss: 0.659\n",
      "epoch: 24 126/196\n",
      "epoch: 24 133/196\n",
      "epoch: 24 140/196\n",
      "epoch: 24 147/196\n",
      "[25,   150] loss: 0.672\n",
      "epoch: 24 154/196\n",
      "epoch: 24 161/196\n",
      "epoch: 24 168/196\n",
      "[25,   175] loss: 0.673\n",
      "epoch: 24 175/196\n",
      "epoch: 24 182/196\n",
      "epoch: 24 189/196\n",
      "Accuracy train 81 %\n",
      "Accuracy test 64 %\n",
      "epoch: 25 0/196\n",
      "epoch: 25 7/196\n",
      "epoch: 25 14/196\n",
      "epoch: 25 21/196\n",
      "[26,    25] loss: 1.180\n",
      "epoch: 25 28/196\n",
      "epoch: 25 35/196\n",
      "epoch: 25 42/196\n",
      "[26,    50] loss: 0.591\n",
      "epoch: 25 49/196\n",
      "epoch: 25 56/196\n",
      "epoch: 25 63/196\n",
      "epoch: 25 70/196\n",
      "[26,    75] loss: 0.609\n",
      "epoch: 25 77/196\n",
      "epoch: 25 84/196\n",
      "epoch: 25 91/196\n",
      "epoch: 25 98/196\n",
      "[26,   100] loss: 0.604\n",
      "epoch: 25 105/196\n",
      "epoch: 25 112/196\n",
      "epoch: 25 119/196\n",
      "[26,   125] loss: 0.621\n",
      "epoch: 25 126/196\n",
      "epoch: 25 133/196\n",
      "epoch: 25 140/196\n",
      "epoch: 25 147/196\n",
      "[26,   150] loss: 0.613\n",
      "epoch: 25 154/196\n",
      "epoch: 25 161/196\n",
      "epoch: 25 168/196\n",
      "[26,   175] loss: 0.646\n",
      "epoch: 25 175/196\n",
      "epoch: 25 182/196\n",
      "epoch: 25 189/196\n",
      "Accuracy train 83 %\n",
      "Accuracy test 63 %\n",
      "epoch: 26 0/196\n",
      "epoch: 26 7/196\n",
      "epoch: 26 14/196\n",
      "epoch: 26 21/196\n",
      "[27,    25] loss: 1.101\n",
      "epoch: 26 28/196\n",
      "epoch: 26 35/196\n",
      "epoch: 26 42/196\n",
      "[27,    50] loss: 0.563\n",
      "epoch: 26 49/196\n",
      "epoch: 26 56/196\n",
      "epoch: 26 63/196\n",
      "epoch: 26 70/196\n",
      "[27,    75] loss: 0.568\n",
      "epoch: 26 77/196\n",
      "epoch: 26 84/196\n",
      "epoch: 26 91/196\n",
      "epoch: 26 98/196\n",
      "[27,   100] loss: 0.605\n",
      "epoch: 26 105/196\n",
      "epoch: 26 112/196\n",
      "epoch: 26 119/196\n",
      "[27,   125] loss: 0.605\n",
      "epoch: 26 126/196\n",
      "epoch: 26 133/196\n",
      "epoch: 26 140/196\n",
      "epoch: 26 147/196\n",
      "[27,   150] loss: 0.596\n",
      "epoch: 26 154/196\n",
      "epoch: 26 161/196\n",
      "epoch: 26 168/196\n",
      "[27,   175] loss: 0.588\n",
      "epoch: 26 175/196\n",
      "epoch: 26 182/196\n",
      "epoch: 26 189/196\n",
      "Accuracy train 84 %\n",
      "Accuracy test 64 %\n",
      "epoch: 27 0/196\n",
      "epoch: 27 7/196\n",
      "epoch: 27 14/196\n",
      "epoch: 27 21/196\n",
      "[28,    25] loss: 1.059\n",
      "epoch: 27 28/196\n",
      "epoch: 27 35/196\n",
      "epoch: 27 42/196\n",
      "[28,    50] loss: 0.562\n",
      "epoch: 27 49/196\n",
      "epoch: 27 56/196\n",
      "epoch: 27 63/196\n",
      "epoch: 27 70/196\n",
      "[28,    75] loss: 0.530\n",
      "epoch: 27 77/196\n",
      "epoch: 27 84/196\n",
      "epoch: 27 91/196\n",
      "epoch: 27 98/196\n",
      "[28,   100] loss: 0.550\n",
      "epoch: 27 105/196\n",
      "epoch: 27 112/196\n",
      "epoch: 27 119/196\n",
      "[28,   125] loss: 0.578\n",
      "epoch: 27 126/196\n",
      "epoch: 27 133/196\n",
      "epoch: 27 140/196\n",
      "epoch: 27 147/196\n",
      "[28,   150] loss: 0.594\n",
      "epoch: 27 154/196\n",
      "epoch: 27 161/196\n",
      "epoch: 27 168/196\n",
      "[28,   175] loss: 0.561\n",
      "epoch: 27 175/196\n",
      "epoch: 27 182/196\n",
      "epoch: 27 189/196\n",
      "Accuracy train 84 %\n",
      "Accuracy test 63 %\n",
      "epoch: 28 0/196\n",
      "epoch: 28 7/196\n",
      "epoch: 28 14/196\n",
      "epoch: 28 21/196\n",
      "[29,    25] loss: 1.035\n",
      "epoch: 28 28/196\n",
      "epoch: 28 35/196\n",
      "epoch: 28 42/196\n",
      "[29,    50] loss: 0.502\n",
      "epoch: 28 49/196\n",
      "epoch: 28 56/196\n",
      "epoch: 28 63/196\n",
      "epoch: 28 70/196\n",
      "[29,    75] loss: 0.527\n",
      "epoch: 28 77/196\n",
      "epoch: 28 84/196\n",
      "epoch: 28 91/196\n",
      "epoch: 28 98/196\n",
      "[29,   100] loss: 0.525\n",
      "epoch: 28 105/196\n",
      "epoch: 28 112/196\n",
      "epoch: 28 119/196\n",
      "[29,   125] loss: 0.504\n",
      "epoch: 28 126/196\n",
      "epoch: 28 133/196\n",
      "epoch: 28 140/196\n",
      "epoch: 28 147/196\n",
      "[29,   150] loss: 0.558\n",
      "epoch: 28 154/196\n",
      "epoch: 28 161/196\n",
      "epoch: 28 168/196\n",
      "[29,   175] loss: 0.543\n",
      "epoch: 28 175/196\n",
      "epoch: 28 182/196\n",
      "epoch: 28 189/196\n",
      "Accuracy train 85 %\n",
      "Accuracy test 63 %\n",
      "epoch: 29 0/196\n",
      "epoch: 29 7/196\n",
      "epoch: 29 14/196\n",
      "epoch: 29 21/196\n",
      "[30,    25] loss: 0.931\n",
      "epoch: 29 28/196\n",
      "epoch: 29 35/196\n",
      "epoch: 29 42/196\n",
      "[30,    50] loss: 0.477\n",
      "epoch: 29 49/196\n",
      "epoch: 29 56/196\n",
      "epoch: 29 63/196\n",
      "epoch: 29 70/196\n",
      "[30,    75] loss: 0.509\n",
      "epoch: 29 77/196\n",
      "epoch: 29 84/196\n",
      "epoch: 29 91/196\n",
      "epoch: 29 98/196\n",
      "[30,   100] loss: 0.491\n",
      "epoch: 29 105/196\n",
      "epoch: 29 112/196\n",
      "epoch: 29 119/196\n",
      "[30,   125] loss: 0.492\n",
      "epoch: 29 126/196\n",
      "epoch: 29 133/196\n",
      "epoch: 29 140/196\n",
      "epoch: 29 147/196\n",
      "[30,   150] loss: 0.505\n",
      "epoch: 29 154/196\n",
      "epoch: 29 161/196\n",
      "epoch: 29 168/196\n",
      "[30,   175] loss: 0.523\n",
      "epoch: 29 175/196\n",
      "epoch: 29 182/196\n",
      "epoch: 29 189/196\n",
      "Accuracy train 86 %\n",
      "Accuracy test 63 %\n",
      "epoch: 30 0/196\n",
      "epoch: 30 7/196\n",
      "epoch: 30 14/196\n",
      "epoch: 30 21/196\n",
      "[31,    25] loss: 0.894\n",
      "epoch: 30 28/196\n",
      "epoch: 30 35/196\n",
      "epoch: 30 42/196\n",
      "[31,    50] loss: 0.481\n",
      "epoch: 30 49/196\n",
      "epoch: 30 56/196\n",
      "epoch: 30 63/196\n",
      "epoch: 30 70/196\n",
      "[31,    75] loss: 0.474\n",
      "epoch: 30 77/196\n",
      "epoch: 30 84/196\n",
      "epoch: 30 91/196\n",
      "epoch: 30 98/196\n",
      "[31,   100] loss: 0.464\n",
      "epoch: 30 105/196\n",
      "epoch: 30 112/196\n",
      "epoch: 30 119/196\n",
      "[31,   125] loss: 0.484\n",
      "epoch: 30 126/196\n",
      "epoch: 30 133/196\n",
      "epoch: 30 140/196\n",
      "epoch: 30 147/196\n",
      "[31,   150] loss: 0.476\n",
      "epoch: 30 154/196\n",
      "epoch: 30 161/196\n",
      "epoch: 30 168/196\n",
      "[31,   175] loss: 0.517\n",
      "epoch: 30 175/196\n",
      "epoch: 30 182/196\n",
      "epoch: 30 189/196\n",
      "Accuracy train 87 %\n",
      "Accuracy test 64 %\n",
      "epoch: 31 0/196\n",
      "epoch: 31 7/196\n",
      "epoch: 31 14/196\n",
      "epoch: 31 21/196\n",
      "[32,    25] loss: 0.839\n",
      "epoch: 31 28/196\n",
      "epoch: 31 35/196\n",
      "epoch: 31 42/196\n",
      "[32,    50] loss: 0.426\n",
      "epoch: 31 49/196\n",
      "epoch: 31 56/196\n",
      "epoch: 31 63/196\n",
      "epoch: 31 70/196\n",
      "[32,    75] loss: 0.440\n",
      "epoch: 31 77/196\n",
      "epoch: 31 84/196\n",
      "epoch: 31 91/196\n",
      "epoch: 31 98/196\n",
      "[32,   100] loss: 0.431\n",
      "epoch: 31 105/196\n",
      "epoch: 31 112/196\n",
      "epoch: 31 119/196\n",
      "[32,   125] loss: 0.445\n",
      "epoch: 31 126/196\n",
      "epoch: 31 133/196\n",
      "epoch: 31 140/196\n",
      "epoch: 31 147/196\n",
      "[32,   150] loss: 0.476\n",
      "epoch: 31 154/196\n",
      "epoch: 31 161/196\n",
      "epoch: 31 168/196\n",
      "[32,   175] loss: 0.488\n",
      "epoch: 31 175/196\n",
      "epoch: 31 182/196\n",
      "epoch: 31 189/196\n",
      "Accuracy train 87 %\n",
      "Accuracy test 64 %\n",
      "epoch: 32 0/196\n",
      "epoch: 32 7/196\n",
      "epoch: 32 14/196\n",
      "epoch: 32 21/196\n",
      "[33,    25] loss: 0.799\n",
      "epoch: 32 28/196\n",
      "epoch: 32 35/196\n",
      "epoch: 32 42/196\n",
      "[33,    50] loss: 0.389\n",
      "epoch: 32 49/196\n",
      "epoch: 32 56/196\n",
      "epoch: 32 63/196\n",
      "epoch: 32 70/196\n",
      "[33,    75] loss: 0.408\n",
      "epoch: 32 77/196\n",
      "epoch: 32 84/196\n",
      "epoch: 32 91/196\n",
      "epoch: 32 98/196\n",
      "[33,   100] loss: 0.422\n",
      "epoch: 32 105/196\n",
      "epoch: 32 112/196\n",
      "epoch: 32 119/196\n",
      "[33,   125] loss: 0.450\n",
      "epoch: 32 126/196\n",
      "epoch: 32 133/196\n",
      "epoch: 32 140/196\n",
      "epoch: 32 147/196\n",
      "[33,   150] loss: 0.437\n",
      "epoch: 32 154/196\n",
      "epoch: 32 161/196\n",
      "epoch: 32 168/196\n",
      "[33,   175] loss: 0.438\n",
      "epoch: 32 175/196\n",
      "epoch: 32 182/196\n",
      "epoch: 32 189/196\n",
      "Accuracy train 88 %\n",
      "Accuracy test 65 %\n",
      "epoch: 33 0/196\n",
      "epoch: 33 7/196\n",
      "epoch: 33 14/196\n",
      "epoch: 33 21/196\n",
      "[34,    25] loss: 0.767\n",
      "epoch: 33 28/196\n",
      "epoch: 33 35/196\n",
      "epoch: 33 42/196\n",
      "[34,    50] loss: 0.407\n",
      "epoch: 33 49/196\n",
      "epoch: 33 56/196\n",
      "epoch: 33 63/196\n",
      "epoch: 33 70/196\n",
      "[34,    75] loss: 0.412\n",
      "epoch: 33 77/196\n",
      "epoch: 33 84/196\n",
      "epoch: 33 91/196\n",
      "epoch: 33 98/196\n",
      "[34,   100] loss: 0.403\n",
      "epoch: 33 105/196\n",
      "epoch: 33 112/196\n",
      "epoch: 33 119/196\n",
      "[34,   125] loss: 0.425\n",
      "epoch: 33 126/196\n",
      "epoch: 33 133/196\n",
      "epoch: 33 140/196\n",
      "epoch: 33 147/196\n",
      "[34,   150] loss: 0.417\n",
      "epoch: 33 154/196\n",
      "epoch: 33 161/196\n",
      "epoch: 33 168/196\n",
      "[34,   175] loss: 0.419\n",
      "epoch: 33 175/196\n",
      "epoch: 33 182/196\n",
      "epoch: 33 189/196\n",
      "Accuracy train 89 %\n",
      "Accuracy test 64 %\n",
      "epoch: 34 0/196\n",
      "epoch: 34 7/196\n",
      "epoch: 34 14/196\n",
      "epoch: 34 21/196\n",
      "[35,    25] loss: 0.718\n",
      "epoch: 34 28/196\n",
      "epoch: 34 35/196\n",
      "epoch: 34 42/196\n",
      "[35,    50] loss: 0.349\n",
      "epoch: 34 49/196\n",
      "epoch: 34 56/196\n",
      "epoch: 34 63/196\n",
      "epoch: 34 70/196\n",
      "[35,    75] loss: 0.384\n",
      "epoch: 34 77/196\n",
      "epoch: 34 84/196\n",
      "epoch: 34 91/196\n",
      "epoch: 34 98/196\n",
      "[35,   100] loss: 0.364\n",
      "epoch: 34 105/196\n",
      "epoch: 34 112/196\n",
      "epoch: 34 119/196\n",
      "[35,   125] loss: 0.384\n",
      "epoch: 34 126/196\n",
      "epoch: 34 133/196\n",
      "epoch: 34 140/196\n",
      "epoch: 34 147/196\n",
      "[35,   150] loss: 0.417\n",
      "epoch: 34 154/196\n",
      "epoch: 34 161/196\n",
      "epoch: 34 168/196\n",
      "[35,   175] loss: 0.411\n",
      "epoch: 34 175/196\n",
      "epoch: 34 182/196\n",
      "epoch: 34 189/196\n",
      "Accuracy train 88 %\n",
      "Accuracy test 64 %\n",
      "epoch: 35 0/196\n",
      "epoch: 35 7/196\n",
      "epoch: 35 14/196\n",
      "epoch: 35 21/196\n",
      "[36,    25] loss: 0.671\n",
      "epoch: 35 28/196\n",
      "epoch: 35 35/196\n",
      "epoch: 35 42/196\n",
      "[36,    50] loss: 0.348\n",
      "epoch: 35 49/196\n",
      "epoch: 35 56/196\n",
      "epoch: 35 63/196\n",
      "epoch: 35 70/196\n",
      "[36,    75] loss: 0.356\n",
      "epoch: 35 77/196\n",
      "epoch: 35 84/196\n",
      "epoch: 35 91/196\n",
      "epoch: 35 98/196\n",
      "[36,   100] loss: 0.358\n",
      "epoch: 35 105/196\n",
      "epoch: 35 112/196\n",
      "epoch: 35 119/196\n",
      "[36,   125] loss: 0.350\n",
      "epoch: 35 126/196\n",
      "epoch: 35 133/196\n",
      "epoch: 35 140/196\n",
      "epoch: 35 147/196\n",
      "[36,   150] loss: 0.367\n",
      "epoch: 35 154/196\n",
      "epoch: 35 161/196\n",
      "epoch: 35 168/196\n",
      "[36,   175] loss: 0.388\n",
      "epoch: 35 175/196\n",
      "epoch: 35 182/196\n",
      "epoch: 35 189/196\n",
      "Accuracy train 89 %\n",
      "Accuracy test 63 %\n",
      "epoch: 36 0/196\n",
      "epoch: 36 7/196\n",
      "epoch: 36 14/196\n",
      "epoch: 36 21/196\n",
      "[37,    25] loss: 0.680\n",
      "epoch: 36 28/196\n",
      "epoch: 36 35/196\n",
      "epoch: 36 42/196\n",
      "[37,    50] loss: 0.325\n",
      "epoch: 36 49/196\n",
      "epoch: 36 56/196\n",
      "epoch: 36 63/196\n",
      "epoch: 36 70/196\n",
      "[37,    75] loss: 0.338\n",
      "epoch: 36 77/196\n",
      "epoch: 36 84/196\n",
      "epoch: 36 91/196\n",
      "epoch: 36 98/196\n",
      "[37,   100] loss: 0.340\n",
      "epoch: 36 105/196\n",
      "epoch: 36 112/196\n",
      "epoch: 36 119/196\n",
      "[37,   125] loss: 0.371\n",
      "epoch: 36 126/196\n",
      "epoch: 36 133/196\n",
      "epoch: 36 140/196\n",
      "epoch: 36 147/196\n",
      "[37,   150] loss: 0.375\n",
      "epoch: 36 154/196\n",
      "epoch: 36 161/196\n",
      "epoch: 36 168/196\n",
      "[37,   175] loss: 0.365\n",
      "epoch: 36 175/196\n",
      "epoch: 36 182/196\n",
      "epoch: 36 189/196\n",
      "Accuracy train 90 %\n",
      "Accuracy test 65 %\n",
      "epoch: 37 0/196\n",
      "epoch: 37 7/196\n",
      "epoch: 37 14/196\n",
      "epoch: 37 21/196\n",
      "[38,    25] loss: 0.653\n",
      "epoch: 37 28/196\n",
      "epoch: 37 35/196\n",
      "epoch: 37 42/196\n",
      "[38,    50] loss: 0.307\n",
      "epoch: 37 49/196\n",
      "epoch: 37 56/196\n",
      "epoch: 37 63/196\n",
      "epoch: 37 70/196\n",
      "[38,    75] loss: 0.320\n",
      "epoch: 37 77/196\n",
      "epoch: 37 84/196\n",
      "epoch: 37 91/196\n",
      "epoch: 37 98/196\n",
      "[38,   100] loss: 0.341\n",
      "epoch: 37 105/196\n",
      "epoch: 37 112/196\n",
      "epoch: 37 119/196\n",
      "[38,   125] loss: 0.351\n",
      "epoch: 37 126/196\n",
      "epoch: 37 133/196\n",
      "epoch: 37 140/196\n",
      "epoch: 37 147/196\n",
      "[38,   150] loss: 0.352\n",
      "epoch: 37 154/196\n",
      "epoch: 37 161/196\n",
      "epoch: 37 168/196\n",
      "[38,   175] loss: 0.320\n",
      "epoch: 37 175/196\n",
      "epoch: 37 182/196\n",
      "epoch: 37 189/196\n",
      "Accuracy train 90 %\n",
      "Accuracy test 65 %\n",
      "epoch: 38 0/196\n",
      "epoch: 38 7/196\n",
      "epoch: 38 14/196\n",
      "epoch: 38 21/196\n",
      "[39,    25] loss: 0.615\n",
      "epoch: 38 28/196\n",
      "epoch: 38 35/196\n",
      "epoch: 38 42/196\n",
      "[39,    50] loss: 0.309\n",
      "epoch: 38 49/196\n",
      "epoch: 38 56/196\n",
      "epoch: 38 63/196\n",
      "epoch: 38 70/196\n",
      "[39,    75] loss: 0.307\n",
      "epoch: 38 77/196\n",
      "epoch: 38 84/196\n",
      "epoch: 38 91/196\n",
      "epoch: 38 98/196\n",
      "[39,   100] loss: 0.317\n",
      "epoch: 38 105/196\n",
      "epoch: 38 112/196\n",
      "epoch: 38 119/196\n",
      "[39,   125] loss: 0.326\n",
      "epoch: 38 126/196\n",
      "epoch: 38 133/196\n",
      "epoch: 38 140/196\n",
      "epoch: 38 147/196\n",
      "[39,   150] loss: 0.316\n",
      "epoch: 38 154/196\n",
      "epoch: 38 161/196\n",
      "epoch: 38 168/196\n",
      "[39,   175] loss: 0.315\n",
      "epoch: 38 175/196\n",
      "epoch: 38 182/196\n",
      "epoch: 38 189/196\n",
      "Accuracy train 90 %\n",
      "Accuracy test 64 %\n",
      "epoch: 39 0/196\n",
      "epoch: 39 7/196\n",
      "epoch: 39 14/196\n",
      "epoch: 39 21/196\n",
      "[40,    25] loss: 0.586\n",
      "epoch: 39 28/196\n",
      "epoch: 39 35/196\n",
      "epoch: 39 42/196\n",
      "[40,    50] loss: 0.311\n",
      "epoch: 39 49/196\n",
      "epoch: 39 56/196\n",
      "epoch: 39 63/196\n",
      "epoch: 39 70/196\n",
      "[40,    75] loss: 0.307\n",
      "epoch: 39 77/196\n",
      "epoch: 39 84/196\n",
      "epoch: 39 91/196\n",
      "epoch: 39 98/196\n",
      "[40,   100] loss: 0.282\n",
      "epoch: 39 105/196\n",
      "epoch: 39 112/196\n",
      "epoch: 39 119/196\n",
      "[40,   125] loss: 0.313\n",
      "epoch: 39 126/196\n",
      "epoch: 39 133/196\n",
      "epoch: 39 140/196\n",
      "epoch: 39 147/196\n",
      "[40,   150] loss: 0.313\n",
      "epoch: 39 154/196\n",
      "epoch: 39 161/196\n",
      "epoch: 39 168/196\n",
      "[40,   175] loss: 0.318\n",
      "epoch: 39 175/196\n",
      "epoch: 39 182/196\n",
      "epoch: 39 189/196\n",
      "Accuracy train 92 %\n",
      "Accuracy test 65 %\n",
      "Training is done\n",
      "Total Training Time (second): 14884.856399774551\n"
     ]
    }
   ],
   "source": [
    "use_gpu = True\n",
    "train_acc = []\n",
    "start = time.time()\n",
    "loss_list = []\n",
    "running_loss = 0\n",
    "total_step = len(trainloader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(trainloader):       \n",
    "        # gpu\n",
    "        if use_gpu:\n",
    "            if torch.cuda.is_available():\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "            \n",
    "        \n",
    "        # backward and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 25 == 24: # print every 500 mini batches\n",
    "            print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i+1,running_loss/25))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "        if i % 7 == 0:\n",
    "            print(\"epoch: {} {}/{}\".format(epoch,i,total_step))\n",
    "\n",
    "\n",
    "    # train\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in trainloader:\n",
    "            images, labels = data\n",
    "\n",
    "            \n",
    "            # gpu\n",
    "            if use_gpu:\n",
    "                if torch.cuda.is_available():\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data,1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(\"Accuracy train %d %%\"%(100*correct/total))\n",
    "    train_acc.append(100*correct/total)\n",
    "\n",
    "    # test\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data      \n",
    "            # gpu\n",
    "            if use_gpu:\n",
    "                if torch.cuda.is_available():\n",
    "                    images, labels = images.to(device), labels.to(device)                \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data,1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(\"Accuracy test %d %%\"%(100*correct/total))\n",
    "    train_acc.append(100*correct/total)\n",
    "    loss_list.append(loss.item())\n",
    "\n",
    "\n",
    "end= time.time()\n",
    "stopWatch = end-start\n",
    "print( \"Training is done\")\n",
    "print('Total Training Time (second):',stopWatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pv2FUk55erp0"
   },
   "outputs": [],
   "source": [
    "classes = ['beaver', 'dolphin', 'otter', 'seal', 'whale', \n",
    "'aquarium' ,'fish', 'ray', 'shark', 'trout', \n",
    "'orchids', 'poppies', 'roses', 'sunflowers', 'tulips', \n",
    "'bottles', 'bowls', 'cans', 'cups', 'plates', \n",
    "'apples', 'mushrooms', 'oranges', 'pears', 'sweet peppers', \n",
    "'clock', 'computer keyboard', 'lamp', 'telephone', 'television', 'bed', 'chair', 'couch', 'table', 'wardrobe', \n",
    "'bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach', \n",
    "'bear', 'leopard', 'lion', 'tiger', 'wolf', \n",
    "'bridge', 'castle', 'house', 'road', 'skyscraper', \n",
    "'cloud', 'forest', 'mountain', 'plain', 'sea', \n",
    "'camel', 'cattle', 'chimpanzee', 'elephant', 'kangaroo', \n",
    "'fox', 'porcupine', 'possum', 'raccoon', 'skunk', \n",
    "'crab', 'lobster', 'snail', 'spider', 'worm', \n",
    "'baby', 'boy', 'girl', 'man', 'woman', \n",
    "'crocodile', 'dinosaur', 'lizard', 'snake', 'turtle', \n",
    "'hamster', 'mouse', 'rabbit', 'shrew', 'squirrel', \n",
    "'maple', 'oak', 'palm', 'pine', 'willow', \n",
    "'bicycle', 'bus', 'motorcycle', 'pickup truck', 'train', \n",
    "'lawn-mower', 'rocket', 'streetcar', 'tank', 'tractor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "oe0fYiBsbrA4",
    "outputId": "357c78c7-a1c5-47ec-e6d0-b5bade237f7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.672507\n",
      "Accuracy score: 0.670600\n"
     ]
    }
   ],
   "source": [
    "def test_label_predictions(model, device, test_loader):\n",
    "    model.eval()\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            prediction = output.argmax(dim=1, keepdim=True)\n",
    "            actuals.extend(target.view_as(prediction))\n",
    "            predictions.extend(prediction)\n",
    "    return [i.item() for i in actuals], [i.item() for i in predictions]\n",
    "\n",
    "actuals, predictions = test_label_predictions(model, device, testloader)\n",
    "print('F1 score: %f' % f1_score(actuals, predictions, average='weighted'))\n",
    "print('Accuracy score: %f' % accuracy_score(actuals, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 803
    },
    "colab_type": "code",
    "id": "zV6jJA9zbrDP",
    "outputId": "af4795b1-9ad5-4638-f464-a2aa7eff3e00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[95  0  0 ...  0  0  0]\n",
      " [ 0 64  0 ...  0  0  2]\n",
      " [ 3  0 58 ...  0  3  0]\n",
      " ...\n",
      " [ 0  0  0 ... 61  1  0]\n",
      " [ 0  0  3 ...  0 67  0]\n",
      " [ 0  0  0 ...  0  0 66]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x1728 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(actuals, predictions)\n",
    "print(cm)\n",
    "fig = plt.figure(figsize=(24,24))\n",
    "ax = fig.add_subplot(211)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + classes)\n",
    "ax.set_yticklabels([''] + classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "m9cYVsALb6EI",
    "outputId": "c0433066-5e46-4f05-da4e-1ac47129eb16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "           beaver    0.76000   0.95000   0.84444       100\n",
      "          dolphin    0.86486   0.64000   0.73563       100\n",
      "            otter    0.54206   0.58000   0.56039       100\n",
      "             seal    0.43750   0.49000   0.46226       100\n",
      "            whale    0.36126   0.69000   0.47423       100\n",
      "         aquarium    0.69231   0.72000   0.70588       100\n",
      "             fish    0.76000   0.76000   0.76000       100\n",
      "              ray    0.63158   0.72000   0.67290       100\n",
      "            shark    0.93407   0.85000   0.89005       100\n",
      "            trout    0.68333   0.82000   0.74545       100\n",
      "          orchids    0.50476   0.53000   0.51707       100\n",
      "          poppies    0.53947   0.41000   0.46591       100\n",
      "            roses    0.68478   0.63000   0.65625       100\n",
      "       sunflowers    0.57658   0.64000   0.60664       100\n",
      "           tulips    0.67901   0.55000   0.60773       100\n",
      "          bottles    0.63934   0.78000   0.70270       100\n",
      "            bowls    0.72043   0.67000   0.69430       100\n",
      "             cans    0.80189   0.85000   0.82524       100\n",
      "             cups    0.61628   0.53000   0.56989       100\n",
      "           plates    0.66000   0.66000   0.66000       100\n",
      "           apples    0.82474   0.80000   0.81218       100\n",
      "        mushrooms    0.83951   0.68000   0.75138       100\n",
      "          oranges    0.60748   0.65000   0.62802       100\n",
      "            pears    0.83505   0.81000   0.82234       100\n",
      "    sweet peppers    0.86207   0.75000   0.80214       100\n",
      "            clock    0.54237   0.64000   0.58716       100\n",
      "computer keyboard    0.68627   0.70000   0.69307       100\n",
      "             lamp    0.58947   0.56000   0.57436       100\n",
      "        telephone    0.78571   0.66000   0.71739       100\n",
      "       television    0.67708   0.65000   0.66327       100\n",
      "              bed    0.70526   0.67000   0.68718       100\n",
      "            chair    0.70588   0.60000   0.64865       100\n",
      "            couch    0.64423   0.67000   0.65686       100\n",
      "            table    0.53543   0.68000   0.59912       100\n",
      "         wardrobe    0.64463   0.78000   0.70588       100\n",
      "              bee    0.44828   0.52000   0.48148       100\n",
      "           beetle    0.88372   0.76000   0.81720       100\n",
      "        butterfly    0.60902   0.81000   0.69528       100\n",
      "      caterpillar    0.70667   0.53000   0.60571       100\n",
      "        cockroach    0.89888   0.80000   0.84656       100\n",
      "             bear    0.46053   0.70000   0.55556       100\n",
      "          leopard    0.84000   0.84000   0.84000       100\n",
      "             lion    0.75000   0.69000   0.71875       100\n",
      "            tiger    0.81522   0.75000   0.78125       100\n",
      "             wolf    0.35971   0.50000   0.41841       100\n",
      "           bridge    0.65854   0.54000   0.59341       100\n",
      "           castle    0.54878   0.45000   0.49451       100\n",
      "            house    0.66265   0.55000   0.60109       100\n",
      "             road    0.90323   0.84000   0.87047       100\n",
      "       skyscraper    0.80612   0.79000   0.79798       100\n",
      "            cloud    0.57143   0.52000   0.54450       100\n",
      "           forest    0.62832   0.71000   0.66667       100\n",
      "         mountain    0.56140   0.64000   0.59813       100\n",
      "            plain    0.79825   0.91000   0.85047       100\n",
      "              sea    0.81053   0.77000   0.78974       100\n",
      "            camel    0.36923   0.48000   0.41739       100\n",
      "           cattle    0.94318   0.83000   0.88298       100\n",
      "       chimpanzee    0.73737   0.73000   0.73367       100\n",
      "         elephant    0.79612   0.82000   0.80788       100\n",
      "         kangaroo    0.51128   0.68000   0.58369       100\n",
      "              fox    0.81818   0.81000   0.81407       100\n",
      "        porcupine    0.69444   0.75000   0.72115       100\n",
      "           possum    0.68000   0.68000   0.68000       100\n",
      "          raccoon    0.68966   0.60000   0.64171       100\n",
      "            skunk    0.62963   0.51000   0.56354       100\n",
      "             crab    0.52381   0.44000   0.47826       100\n",
      "          lobster    0.70192   0.73000   0.71569       100\n",
      "            snail    0.63855   0.53000   0.57923       100\n",
      "           spider    0.91000   0.91000   0.91000       100\n",
      "             worm    0.84444   0.76000   0.80000       100\n",
      "             baby    0.68269   0.71000   0.69608       100\n",
      "              boy    0.76923   0.80000   0.78431       100\n",
      "             girl    0.37963   0.41000   0.39423       100\n",
      "              man    0.52941   0.45000   0.48649       100\n",
      "            woman    0.40541   0.45000   0.42654       100\n",
      "        crocodile    0.75248   0.76000   0.75622       100\n",
      "         dinosaur    0.92222   0.83000   0.87368       100\n",
      "           lizard    0.77143   0.54000   0.63529       100\n",
      "            snake    0.63529   0.54000   0.58378       100\n",
      "           turtle    0.78481   0.62000   0.69274       100\n",
      "          hamster    0.52830   0.56000   0.54369       100\n",
      "            mouse    0.68421   0.52000   0.59091       100\n",
      "           rabbit    0.80702   0.92000   0.85981       100\n",
      "            shrew    0.77500   0.62000   0.68889       100\n",
      "         squirrel    0.63964   0.71000   0.67299       100\n",
      "            maple    0.79775   0.71000   0.75132       100\n",
      "              oak    0.59836   0.73000   0.65766       100\n",
      "             palm    0.72727   0.72000   0.72362       100\n",
      "             pine    0.80000   0.80000   0.80000       100\n",
      "           willow    0.78723   0.74000   0.76289       100\n",
      "          bicycle    0.82500   0.66000   0.73333       100\n",
      "              bus    0.69298   0.79000   0.73832       100\n",
      "       motorcycle    0.61111   0.55000   0.57895       100\n",
      "     pickup truck    0.53271   0.57000   0.55072       100\n",
      "            train    0.92135   0.82000   0.86772       100\n",
      "       lawn-mower    0.70000   0.56000   0.62222       100\n",
      "           rocket    0.67857   0.38000   0.48718       100\n",
      "        streetcar    0.81333   0.61000   0.69714       100\n",
      "             tank    0.50758   0.67000   0.57759       100\n",
      "          tractor    0.57391   0.66000   0.61395       100\n",
      "\n",
      "         accuracy                        0.67060     10000\n",
      "        macro avg    0.68418   0.67060   0.67251     10000\n",
      "     weighted avg    0.68418   0.67060   0.67251     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    " print(classification_report(actuals, predictions, target_names=classes, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UGOiuFRlGjqY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ResNet50 on Cifar 100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "17ab7edf3843410182a0e1b1d2f992fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b89d0abb7194008bbce622ed5687265",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3dd48e39941c4b83a6bb5181fccf9554",
      "value": 1
     }
    },
    "3dd48e39941c4b83a6bb5181fccf9554": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "74ff845e25604f548f47fea8b629a4aa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b89d0abb7194008bbce622ed5687265": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8bffefb88a124d4a9c0c209b1bde1950": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_17ab7edf3843410182a0e1b1d2f992fe",
       "IPY_MODEL_a923df6caf654c10adc55ff9c519bb2d"
      ],
      "layout": "IPY_MODEL_74ff845e25604f548f47fea8b629a4aa"
     }
    },
    "a15ab6aea3ca43809d12366dd3563b2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a923df6caf654c10adc55ff9c519bb2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc5d9dfdd01546b281c589d9b3e93959",
      "placeholder": "​",
      "style": "IPY_MODEL_a15ab6aea3ca43809d12366dd3563b2c",
      "value": " 169009152/? [00:30&lt;00:00, 17839992.67it/s]"
     }
    },
    "fc5d9dfdd01546b281c589d9b3e93959": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
